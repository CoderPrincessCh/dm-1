import requests
import re
import csv
from time import sleep
from html import unescape
import matplotlib.pyplot as plt
import pandas as pd
import matplotlib
import certifi
import openpyxl
from openpyxl.styles import Font, Alignment, PatternFill
from openpyxl.utils import get_column_letter
import uuid
from datetime import datetime

DRAMALIST_API = "https://www.missevan.com/dramaapi/filter"
# DRAMALIST_API = "https://app.missevan.com/drama/filter"
SEARCH_API = "https://www.missevan.com/dramaapi/search"
# app_headers = {
#     "User-Agent": "MissEvanApp/6.4.0 (Android;9;Asus ASUS_I003DD ASUS_I003DD)",
#     "channel": "missevan",
#     "Accept": "application/json",
#     "Cookie": "equip_id=680089c7-c0e7-af6a-acab-1d41fcc32ea0; buvid=XZ223262BF341FCFF274134D12230B4746235; device_token=v1|5WodYAe7rq9IOf2uQRqVC91Rt5tUEKq3IWEle1lIumjiWuudxLWrmn4znl182dEeP",
#     "Authorization": "MissEvan pwRVqLHmZB0T2CmfYNwRmbVBz5iqA2Op2xm2cdgFzfs=",
#     "X-M-Date": datetime.utcnow().isoformat(timespec='milliseconds') + "Z",
#     "X-M-Nonce": str(uuid.uuid4()),
#     "bili-bridge-engine": "cronet",
#     "Accept-Encoding": "gzip, deflate, br"
# }
headers = {
    "User-Agent": "Mozilla/5.0"
}
requests.get(DRAMALIST_API, headers=headers, verify=certifi.where())

def clean_html(raw_html):
    # å»é™¤ HTML æ ‡ç­¾
    text = re.sub(r"<.*?>", "", raw_html)
    return unescape(text)

def extract_producers(abstract_html: str):
    text = clean_html(abstract_html)

    # æå–â€œè”åˆå‡ºå“â€ã€â€œå‡ºå“â€ã€â€œè”åˆåˆ¶ä½œâ€ ç­‰å¥å­
    match = re.search(r"(çŒ«è€³FM.*?å‡ºå“|.*?è”åˆå‡ºå“|.*?å‡ºå“)", text)
    producers = match.group(0).strip() if match else ""

    has_maoer = "çŒ«è€³FM" in producers
    return producers, "æ˜¯" if has_maoer else "å¦"

def extract_all_producers(abstract_html: str):
    text = clean_html(abstract_html)

    # å…ˆæ‰¾æ‰€æœ‰åŒ…å«â€œå‡ºå“â€ã€â€œè”åˆåˆ¶ä½œâ€ç­‰å…³é”®è¯çš„çŸ­å¥
    # è¿™é‡ŒåŒ¹é…è¿ç»­ä¸è¶…è¿‡20ä¸ªéæ¢è¡Œå­—ç¬¦ï¼Œä¸”åŒ…å«â€œå‡ºå“â€æˆ–â€œåˆ¶ä½œâ€ç›¸å…³å…³é”®è¯
    pattern = r"([\u4e00-\u9fa5A-Za-z0-9Â·ã€ï¼Œ, ]{1,30}?(è”åˆå‡ºå“|è”åˆåˆ¶ä½œ|å‡ºå“|åˆ¶ä½œ|åä½œ)[\u4e00-\u9fa5A-Za-z0-9Â·ã€ï¼Œ, ]{0,10})"
    
    matches = re.findall(pattern, text)

    # matches æ˜¯ (åŒ¹é…çŸ­å¥, å…³é”®è¯) çš„å…ƒç»„åˆ—è¡¨ï¼Œå–ç¬¬0ä¸ªå…ƒç´ 
    producers_list = [m[0].strip(" ,ï¼Œ") for m in matches]

    # å»é‡ä¸”ä¿æŒé¡ºåº
    seen = set()
    producers = []
    for p in producers_list:
        if p not in seen:
            seen.add(p)
            producers.append(p)

    for p in producers_list:
        # è¿‡æ»¤åŒ…å«ä»¥ä¸‹è¯çš„æ¡ç›®
        if ("åŸè‘—" in p) or ("å¹¿æ’­å‰§" in p) or ("ä½œè€…" in p):
            continue
        if p not in seen:
            seen.add(p)
            producers.append(p)

    # æ˜¯å¦å«â€œçŒ«è€³FMâ€
    has_maoer = any("çŒ«è€³FM" in p for p in producers)

    # è¿”å›æ‰€æœ‰å‡ºå“æ–¹åˆ—è¡¨ï¼Œå’Œâ€œæ˜¯å¦çŒ«è€³FMâ€æ ‡è¯†
    return producers, "æ˜¯" if has_maoer else "å¦"

def add_category_by_abstract(abstract_html: str):
    """
    æ ¹æ® abstract å­—æ®µç»™æ¯æ¡æ•°æ®åŠ  category å­—æ®µï¼š
    åŒ…å«â€œå¹¿æ’­å‰§â€å°±æ˜¯å¹¿æ’­å‰§ï¼Œå…¶ä»–å½’ä¸ºâ€œæœ‰å£°å‰§åŠå…¶ä»–â€
    """
    text = clean_html(abstract_html)
    if "æœ‰å£°å‰§" in text:
        return "æœ‰å£°å‰§"
    else:
        return "å¹¿æ’­å‰§"


def chinese_to_arabic(cn: str) -> int | None:
    cn_num = {
        'é›¶': 0, 'ä¸€': 1, 'äºŒ': 2, 'ä¸¤': 2, 'ä¸‰': 3, 'å››': 4,
        'äº”': 5, 'å…­': 6, 'ä¸ƒ': 7, 'å…«': 8, 'ä¹': 9
    }
    cn_unit = {'å': 10, 'ç™¾': 100, 'åƒ': 1000}

    result = 0
    unit = 1  # å½“å‰å•ä½
    tmp = 0   # ä¸´æ—¶æ•°å­—å­˜å‚¨ï¼Œç”¨äºå¤„ç†ä¸ªä½å’Œå•ä½ç»“åˆ

    for i in reversed(cn):
        if i in cn_unit:
            unit = cn_unit[i]
            if tmp == 0:  # å¦‚â€œåâ€å•ç‹¬ï¼Œä»£è¡¨10
                tmp = 1
            result += tmp * unit
            tmp = 0
            unit = 1
        elif i in cn_num:
            tmp = cn_num[i]
        else:
            return None  # é‡åˆ°ä¸è®¤è¯†çš„å­—ç¬¦è¿”å›None

    result += tmp * unit
    return result if result != 0 else None


def extract_episode_counts(abstract_html: str):
    text = clean_html(abstract_html)

    main_eps = None
    total_eps = None
    extra_eps = 0

    # ä¼˜å…ˆåŒ¹é…ï¼š9é›†æ­£å‰§ + 4ä¸ªç•ªå¤–
    combo_match = re.search(
        r"(\d+)\s*[é›†è¯æœŸ]+\s*æ­£å‰§\s*[,ã€ï¼Œ\s]*\s*(\d+)\s*[é›†è¯æœŸ]*\s*(ç•ªå¤–|èŠ±çµ®|å°å‰§åœº|ç¦åˆ©)?",
        text
    )
    if combo_match:
        main_eps = int(combo_match.group(1))
        extra_eps = int(combo_match.group(2))
        total_eps = main_eps + extra_eps

    # åŒ¹é…ï¼šå…±Xé›†æ­£å‰§ / æ­£å‰§Xé›†
    if main_eps is None:
        alt_main_match = re.search(r"(?:æ­£å‰§\s*å…±?|å…±\s*)?(\d+)\s*[é›†è¯æœŸ]\s*æ­£å‰§", text)
        if alt_main_match:
            main_eps = int(alt_main_match.group(1))
            total_eps = main_eps

    # åŒ¹é…ï¼š295+é›†æ­£å‰§
    if main_eps is None:
        plus_match = re.search(r"(?:æ­£å‰§\s*å…±|å…±)?\s*(\d+)\+\s*[é›†è¯æœŸ]\s*æ­£å‰§", text)
        if plus_match:
            main_eps = int(plus_match.group(1))
            total_eps = None  # ä¸ç¡®å®š

    # åŒ¹é…ï¼šå…±Xé›†ï¼ˆå«ç•ªå¤–ï¼‰
    if main_eps is None:
        total_match = re.search(r"å…±\s*(\d+)\s*[é›†è¯æœŸ]", text)
        if total_match:
            total_eps = int(total_match.group(1))

            extra_match = re.search(r"å«\s*(\d+)\s*[é›†è¯æœŸ]?.*?(ç•ªå¤–|èŠ±çµ®|å°å‰§åœº|ç¦åˆ©)", text)
            extra_eps = int(extra_match.group(1)) if extra_match else 0

            main_eps = total_eps - extra_eps

    # åŒ¹é…ï¼šå…±XæœŸ / ç¬¬ä¸€å­£Xé›†
    if main_eps is None:
        fallback_match = re.search(r"(?:å…±|ç¬¬ä¸€å­£)?\s*(\d{1,4})\s*[é›†æœŸè¯]", text)
        if fallback_match:
            main_eps = int(fallback_match.group(1))
            total_eps = main_eps

    # åŒ¹é…ï¼šä¸­æ–‡æ•°å­— â€œåå…­é›†â€
    if main_eps is None:
      zh_match = re.search(r"(?:å…±)?([é›¶ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒä¸¤]+)[é›†æœŸè¯]", text)
      if zh_match:
          cn_number = zh_match.group(1)
          main_eps = chinese_to_arabic(cn_number)
          total_eps = main_eps


    # åŒ¹é…ï¼šâ€œåŒ…å«æ­£å‰§14æœŸâ€
    # æ–°å¢ï¼šæ›´å®½æ¾åŒ¹é…â€œåŒ…å«æ­£å‰§14æœŸã€å°å‰§åœº...â€ç»“æ„
    if main_eps is None:
        contain_main_match = re.search(r"åŒ…å«\s*æ­£å‰§\s*([0-9é›¶ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒä¸¤]+)[é›†è¯æœŸ]", text)
        if contain_main_match:
            raw = contain_main_match.group(1)
            main_eps = int(raw) if raw.isdigit() else chinese_to_arabic(raw)
            total_eps = main_eps  # å…ˆå‡è®¾æ€»é›†æ•°ç­‰äºæ­£å‰§æ•°


    # åŒ¹é…â€œå‰Xé›†å…è´¹â€/â€œé¦–Xé›†é™å…â€
    free_match = re.search(r"(?:å‰|é¦–)\s*([0-9é›¶ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒä¸¤]+)\s*[é›†è¯æœŸ]?.*?(å…è´¹|é™å…)", text)
    # free_eps = int(free_match.group(1)) if free_match else 0
    if free_match:
        raw = free_match.group(1)
        if raw.isdigit():
            free_eps = int(raw)
        else:
            free_eps = chinese_to_arabic(raw) or 0
    else:
        free_eps = 0


    # æ–°å¢åŒ¹é…ï¼šç¬¬ä¸€å­£XæœŸ / ç¬¬ä¸€å­£Xé›†ï¼Œå…¼å®¹â€œç¬¬ä¸€å­£10æœŸï¼Œå…±229é’»çŸ³â€è¿™ç§æ ¼å¼
    if main_eps is None:
        season_match = re.search(r"ç¬¬ä¸€å­£\s*(\d{1,4})\s*[é›†æœŸè¯]", text)
        if season_match:
            main_eps = int(season_match.group(1))
            total_eps = main_eps

    # ä»˜è´¹é›† = æ­£å‰§ - å…è´¹
    if main_eps is not None:
        paid_eps = max(main_eps - free_eps, 0)
    else:
        paid_eps = None

    return total_eps, main_eps, paid_eps

# webè¯·æ±‚
def fetch_drama_list_page(page: int):
    # url = f"{DRAMALIST_API}?filters=0_0_0_0&page={page}"
    # url = f"{DRAMALIST_API}?filters=2_0_4_0_0_2_0&page={page}"
    url = f"{DRAMALIST_API}?filters=0_0_0_0&page={page}&order=1&page_size=20&type=3"
    resp = requests.get(url, headers=headers, timeout=10)
    resp.raise_for_status()
    return resp.json()["info"]["Datas"]

# APPè¯·æ±‚ï¼ˆå¤±è´¥
# def fetch_drama_list_page(page: int):
#     url = f"{DRAMALIST_API}?filters=2_0_4_0_0_2_0&page={page}"
#     resp = requests.get(url, headers=app_headers, timeout=10, verify=False)
#     resp.raise_for_status()
#     return resp.json()["info"]["Datas"]

def extract_cover_date(cover_url: str) -> str:
    """
    ä» cover é“¾æ¥ä¸­æå–æ—¥æœŸï¼Œæ ¼å¼å¦‚ï¼š202501ï¼Œè¿”å› "2025-01"
    """
    match = re.search(r"/(\d{6})/", cover_url)
    if match:
        raw = match.group(1)
        year = raw[:4]
        month = raw[4:]
        return f"{year}-{month}"
    return "æœªçŸ¥"


def fetch_all_seasons_by_name(drama_name: str):
    """
    æ ¹æ®ä¸»å‰§åæœç´¢ï¼Œè¿”å›æ‰€æœ‰åŒ¹é…å‰§ï¼ˆåŒ…å«å¤šå­£ï¼‰ï¼Œ
    è¿”å›åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ æ˜¯ dictï¼ŒåŒ…å«ä»·æ ¼ã€é›†æ•°ã€åå­—ã€abstract
    """
    params = {"s": drama_name, "page": 1}
    resp = requests.get(SEARCH_API, headers=headers, params=params, timeout=10)
    resp.raise_for_status()
    candidates = resp.json()["info"]["Datas"]

    results = []
    for item in candidates:
        name = item.get("name", "")
        price_diamond = item.get("price", 0)

        # åªä¿ç•™ä»·æ ¼å¤§äº0çš„
        if price_diamond <= 0:
            continue

        # è¿‡æ»¤åªè¦åŒ…å«ä¸»å‰§åçš„ç»“æœ
        if drama_name in name:
            abstract = item.get("abstract", "")
            producers, is_maoer = extract_all_producers(abstract)
            total_eps, main_eps, paid_eps = extract_episode_counts(abstract)
            cover_url = item.get("cover", "")
            cover_date = extract_cover_date(cover_url)
            category = add_category_by_abstract(abstract)
            results.append({
                "å‰§å": name,
                "ä»·æ ¼é’»çŸ³": price_diamond,
                "æ€»é›†æ•°": total_eps,
                "æ€»é›†æ•°ï¼ˆä¸å«ç•ªå¤–ï¼‰": main_eps,
                "abstract": abstract,
                "ä»˜è´¹é›†æ•°": paid_eps,
                "å¤‡æ³¨": "ï¼›".join(producers),
                "æ˜¯å¦çŒ«è€³FM": is_maoer,
                "å°é¢æ—¶é—´": cover_date,
                "ç±»å‹": category,
            })

    if not results:
        print(f"âš ï¸ æœªæ‰¾åˆ°åŒ¹é…å‰§ç›®: {drama_name}")
    return results


def clean_drama_name(name: str) -> str:
    """
    å»æ‰â€œç¬¬Xå­£â€ã€â€œä¸Šå­£â€ã€â€œä¸‹å­£â€ã€â€œå…¨ä¸€å­£â€ã€â€œåˆé›†â€ç­‰åç¼€ï¼Œä»…ä¿ç•™ä¸»å‰§å
    """
    return re.sub(r"[\sÂ·]*(ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å1234567890]+å­£|å…¨[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å1234567890]+å­£|ä¸Šå­£|ä¸‹å­£|åˆé›†)$", "", name).strip()


def process_one_drama(drama):
    raw_name = drama["name"]
    drama_name = clean_drama_name(raw_name)
    drama_id = drama["id"]
    # cover_url = drama.get("cover", "")
    # cover_date = extract_cover_date(cover_url)

    all_seasons = fetch_all_seasons_by_name(drama_name)
    results = []

    if not all_seasons:
        print(f"âš ï¸ {raw_name} æœªæ‰¾åˆ°ä»»ä½•å­£ï¼Œè·³è¿‡")
        return []
    
    # è·å–ç¬¬ä¸€å­£å°é¢æ—¶é—´
    first_season_time = all_seasons[0].get("å°é¢æ—¶é—´", "æœªçŸ¥")

    for season in all_seasons:
        cover_date = season.get("å°é¢æ—¶é—´", "æœªçŸ¥")
        # è·³è¿‡ç®€ä»‹ä¸­åŒ…å«â€œåŸåˆ›â€çš„å‰§
        if "åŸåˆ›" in season["abstract"]:
            continue
        price_rmb = season["ä»·æ ¼é’»çŸ³"] / 10
        main_eps = season["æ€»é›†æ•°ï¼ˆä¸å«ç•ªå¤–ï¼‰"]
        paid_eps = season["ä»˜è´¹é›†æ•°"]
        producers = season.get("å¤‡æ³¨", [])
        is_maoer = season.get("æ˜¯å¦çŒ«è€³FM", "å¦")
        category = season.get("ç±»å‹")

        # è·³è¿‡ä»·æ ¼ä½äº 50 é’»çŸ³ï¼ˆå³ 5 å…ƒï¼‰çš„å‰§
        if season["ä»·æ ¼é’»çŸ³"] < 50:
            continue

        if paid_eps is None or paid_eps == 0:
            unit_price = 0
        else:
            unit_price = round(price_rmb / paid_eps, 2)

        if main_eps is None or main_eps == 0:
            avg_price = 0
            print(f"âš ï¸ {season['å‰§å']} æ— æ³•è®¡ç®—é›†å‡ä»·ï¼ˆæ­£å‰§æœªçŸ¥æˆ–ä¸º0ï¼‰")
        else:
            avg_price = round(price_rmb / main_eps, 2)

        results.append({
            "å°é¢æ—¶é—´": cover_date,
            "å‰§å": season["å‰§å"],
            "å‰§ID": drama_id,
            "æ€»é›†æ•°ï¼ˆå«ç•ªå¤–ï¼‰": season["æ€»é›†æ•°"] if season["æ€»é›†æ•°"] else "æœªçŸ¥",
            "æ€»é›†æ•°ï¼ˆä¸å«ç•ªå¤–ï¼‰": main_eps if main_eps else "æœªçŸ¥",
            "æ€»ä»·ï¼ˆå…ƒï¼‰": round(price_rmb, 2),
            "é›†å‡ä»·ï¼ˆå…ƒï¼‰": avg_price,
            "ä»˜è´¹é›†æ•°": paid_eps if paid_eps else "æœªçŸ¥",
            "ä»˜è´¹é›†å•ä»·ï¼ˆå…ƒï¼‰": unit_price,
            "å¤‡æ³¨": producers,
            "æ˜¯å¦çŒ«è€³FM": is_maoer,
            "first_season_time": first_season_time,  # æ–°å¢å­—æ®µï¼Œåªç”¨äºæ’åº
            "ç±»å‹": category,
            
        })

    return results

def split_by_abstract_type(results):
    broadcast_drama = []
    other_drama = []

    for item in results:
        abstract = item.get("abstract", "")
        if "å¹¿æ’­å‰§" in abstract:
            broadcast_drama.append(item)
        else:
            # æœ‰å£°å‰§æˆ–è€…å…¶ä»–éƒ½å½’åˆ°è¿™é‡Œ
            other_drama.append(item)
    
    return broadcast_drama, other_drama

def apply_style_to_sheet(ws):
    header_font = Font(bold=True)
    center_wrap = Alignment(horizontal="center", vertical="center", wrap_text=True)
    year_row_fill = PatternFill("solid", fgColor="D9E1F2")  # æ·¡è“è‰²

    # æ ‡é¢˜è¡Œæ ·å¼
    for cell in ws[1]:
        cell.font = header_font
        cell.alignment = center_wrap

    max_col = ws.max_column

    row_idx = 2
    while row_idx <= ws.max_row:
        first_cell_value = ws.cell(row=row_idx, column=1).value
        if isinstance(first_cell_value, str) and first_cell_value.startswith("ğŸ“… "):
            ws.merge_cells(start_row=row_idx, start_column=1, end_row=row_idx, end_column=max_col)
            merged_cell = ws.cell(row=row_idx, column=1)
            merged_cell.font = header_font
            merged_cell.alignment = center_wrap
            merged_cell.fill = year_row_fill
            row_idx += 1
        else:
            for col_idx in range(1, max_col + 1):
                ws.cell(row=row_idx, column=col_idx).alignment = center_wrap
            row_idx += 1

    # è‡ªåŠ¨åˆ—å®½
    for col in ws.columns:
        max_len = 0
        col_letter = get_column_letter(col[0].column)
        for cell in col:
            try:
                value = str(cell.value) if cell.value is not None else ""
                max_len = max(max_len, len(value))
            except:
                pass
        ws.column_dimensions[col_letter].width = max_len + 2


def export_with_style(filename, df):
    wb = openpyxl.load_workbook(filename)
    ws = wb.active

    header_font = Font(bold=True)
    center_wrap = Alignment(horizontal="center", vertical="center", wrap_text=True)
    year_row_fill = PatternFill("solid", fgColor="D9E1F2")  # æ·¡è“è‰²

    # è®¾ç½®æ ‡é¢˜è¡Œæ ·å¼ï¼ˆç¬¬ä¸€è¡Œï¼‰
    for cell in ws[1]:
        cell.font = header_font
        cell.alignment = center_wrap

    max_col = ws.max_column

    # ä»ç¬¬äºŒè¡Œå¼€å§‹è®¾ç½®æ ·å¼ï¼Œåˆå¹¶â€œğŸ“… å¹´ä»½â€è¡Œ
    row_idx = 2
    while row_idx <= ws.max_row:
        first_cell_value = ws.cell(row=row_idx, column=1).value
        if isinstance(first_cell_value, str) and first_cell_value.startswith("ğŸ“… "):
            ws.merge_cells(start_row=row_idx, start_column=1, end_row=row_idx, end_column=max_col)
            merged_cell = ws.cell(row=row_idx, column=1)
            merged_cell.font = header_font
            merged_cell.alignment = center_wrap
            merged_cell.fill = year_row_fill
            row_idx += 1
        else:
            # æ™®é€šæ•°æ®è¡Œï¼Œå…¨éƒ¨å±…ä¸­
            for col_idx in range(1, max_col + 1):
                ws.cell(row=row_idx, column=col_idx).alignment = center_wrap
            row_idx += 1

    # è‡ªåŠ¨è°ƒæ•´åˆ—å®½
    for col in ws.columns:
        max_len = 0
        col_letter = get_column_letter(col[0].column)
        for cell in col:
            try:
                value = str(cell.value) if cell.value is not None else ""
                max_len = max(max_len, len(value))
            except:
                pass
        ws.column_dimensions[col_letter].width = max_len + 2

    wb.save(filename)
    print(f"æ ·å¼åº”ç”¨å®Œæˆï¼š{filename}")

def main(export_csv=True):
    result = []
    page = 1
    max_pages = 200  # æœ€å¤§æŠ“å–é¡µæ•°é™åˆ¶
    processed_main_titles = set()

    while True:
        # if page > max_pages:
        #     print(f"ğŸ“¢ å·²è¾¾åˆ°æœ€å¤§æŠ“å–é¡µæ•° {max_pages}ï¼Œç»“æŸæŠ“å–ã€‚")
        #     break

        print(f"ğŸ“¦ æ­£åœ¨æŠ“å–ç¬¬ {page} é¡µå‰§åˆ—è¡¨...")
        try:
            dramas = fetch_drama_list_page(page)
        except Exception as e:
            print(f"â— ç¬¬ {page} é¡µè·å–å¤±è´¥ï¼š{e}")
            break

        if not dramas:
            print("âœ… æŠ“å–å®Œæ¯•")
            break

        for drama in dramas:
          if drama.get("pay_type") != 2:
              continue
            
          # è·å–ä¸»å‰§åï¼ˆå»æ‰ç¬¬å‡ å­£ï¼‰
          raw_name = drama.get("name", "")
          main_title = clean_drama_name(raw_name)

          if main_title in processed_main_titles:
              continue  # âœ… å·²å¤„ç†è¿‡è¯¥ä¸»å‰§ï¼Œè·³è¿‡
          processed_main_titles.add(main_title)

          season_results = process_one_drama(drama)
          if season_results:
              result.extend(season_results)
              for data in season_results:
                  print(f"âœ… {data['å°é¢æ—¶é—´']} | {data['å‰§å']} | ç±»å‹ï¼š{data['ç±»å‹']} | æ€»é›†æ•°ï¼ˆå«ç•ªå¤–ï¼‰{data['æ€»é›†æ•°ï¼ˆå«ç•ªå¤–ï¼‰']}é›† | æ€»é›†æ•°ï¼ˆä¸å«ç•ªå¤–ï¼‰{data['æ€»é›†æ•°ï¼ˆä¸å«ç•ªå¤–ï¼‰']}é›† | ä»˜è´¹{data['ä»˜è´¹é›†æ•°']}é›† | æ€»ä»·ï¼ˆå…ƒï¼‰ï¼šÂ¥{data['æ€»ä»·ï¼ˆå…ƒï¼‰']} | é›†å‡ä»·ï¼šÂ¥{data['é›†å‡ä»·ï¼ˆå…ƒï¼‰']} | ä»˜è´¹é›†å•ä»·ï¼šÂ¥{data['ä»˜è´¹é›†å•ä»·ï¼ˆå…ƒï¼‰']} | å¤‡æ³¨ï¼š{data['å¤‡æ³¨']}")
          sleep(0.3)
        page += 1
        sleep(0.5)

        excel_file = "å¹¿æ’­å‰§_æœ‰å£°å‰§_åˆ†è¡¨å¯¼å‡º.xlsx"
        if export_csv and result:
            df = pd.DataFrame(result)
    
            # ç»Ÿä¸€æ·»åŠ å¹´ä»½åˆ—å¹¶æ’åº
            df["å¹´ä»½"] = df["first_season_time"].str[:4]
            df = df.sort_values(by=["first_season_time", "å°é¢æ—¶é—´"])
    
            export_cols = ["å°é¢æ—¶é—´", "å‰§å", "æ€»ä»·ï¼ˆå…ƒï¼‰", "æ€»é›†æ•°ï¼ˆä¸å«ç•ªå¤–ï¼‰",
                           "é›†å‡ä»·ï¼ˆå…ƒï¼‰", "ä»˜è´¹é›†æ•°", "ä»˜è´¹é›†å•ä»·ï¼ˆå…ƒï¼‰"]
    
            def build_grouped_rows(sub_df):
                grouped_rows = []
                grouped = sub_df.groupby("å¹´ä»½")
                for year, group in grouped:
                    # æ’å…¥å¹´ä»½æ ‡é¢˜è¡Œ
                    grouped_rows.append({"å‰§å": f"ğŸ“… {year} å¹´"})
                    grouped_rows.extend(group[export_cols].to_dict(orient="records"))
                return grouped_rows
    
            # å¹¿æ’­å‰§
            radio_df = df[df["ç±»å‹"] == "å¹¿æ’­å‰§"]
            radio_rows = build_grouped_rows(radio_df)
            radio_final_df = pd.DataFrame(radio_rows)
    
            # æœ‰å£°å‰§åŠå…¶ä»–
            other_df = df[df["ç±»å‹"] != "å¹¿æ’­å‰§"]
            other_rows = build_grouped_rows(other_df)
            other_final_df = pd.DataFrame(other_rows)
    
            with pd.ExcelWriter(excel_file, engine='openpyxl') as writer:
                radio_final_df.to_excel(writer, sheet_name="å¹¿æ’­å‰§", index=False)
                other_final_df.to_excel(writer, sheet_name="æœ‰å£°å‰§åŠå…¶ä»–", index=False)
    
        # åˆ†åˆ«åº”ç”¨æ ·å¼åˆ°ä¸¤ä¸ª sheet
        # openpyxl é»˜è®¤æ‰“å¼€çš„æ˜¯ç¬¬ä¸€ä¸ª sheetï¼Œç¬¬äºŒä¸ªéœ€è¦åˆ‡æ¢
        wb = openpyxl.load_workbook(excel_file)

        # åº”ç”¨å¹¿æ’­å‰§sheetæ ·å¼
        ws_radio = wb["å¹¿æ’­å‰§"]
        apply_style_to_sheet(ws_radio)

        # åº”ç”¨æœ‰å£°å‰§åŠå…¶ä»–sheetæ ·å¼
        ws_other = wb["æœ‰å£°å‰§åŠå…¶ä»–"]
        apply_style_to_sheet(ws_other)

        wb.save(excel_file)
        print(f"ğŸ“„ åˆ†ç»„å¯¼å‡ºå¹¶ç¾åŒ–å®Œæˆï¼š{excel_file}")

    return result

# è®¾ç½®ä¸­æ–‡å­—ä½“æ”¯æŒï¼ˆå¦‚ä½¿ç”¨å¾®è½¯é›…é»‘ï¼‰
matplotlib.rcParams['font.family'] = ['Microsoft YaHei']
matplotlib.rcParams['axes.unicode_minus'] = False  # è´Ÿå·æ­£å¸¸æ˜¾ç¤º

# def draw_price_charts(csv_file="å…¨éƒ¨ä»˜è´¹å¹¿æ’­å‰§_é›†å‡ä»·ç»Ÿè®¡.csv"):
# # def draw_price_charts(csv_file="å…¨éƒ¨ä»˜è´¹å¹¿æ’­å‰§_é›†å‡ä»·ç»Ÿè®¡.csv", top_n=30):
#     df = pd.read_csv(csv_file)

#     # è¿‡æ»¤æ‰æ— æ³•è½¬æ•°å­—çš„é¡¹
#     df = df[pd.to_numeric(df["é›†å‡ä»·ï¼ˆå…ƒï¼‰"], errors='coerce').notna()]
#     df = df[pd.to_numeric(df["ä»˜è´¹é›†å•ä»·ï¼ˆå…ƒï¼‰"], errors='coerce').notna()]

#     df["é›†å‡ä»·ï¼ˆå…ƒï¼‰"] = df["é›†å‡ä»·ï¼ˆå…ƒï¼‰"].astype(float)
#     df["ä»˜è´¹é›†å•ä»·ï¼ˆå…ƒï¼‰"] = df["ä»˜è´¹é›†å•ä»·ï¼ˆå…ƒï¼‰"].astype(float)

#     # æ’åºåå–å‰ N é¡¹
#     top_avg = df.sort_values("é›†å‡ä»·ï¼ˆå…ƒï¼‰", ascending=False).head(top_n)
#     top_unit = df.sort_values("ä»˜è´¹é›†å•ä»·ï¼ˆå…ƒï¼‰", ascending=False).head(top_n)

#     # âœ… å›¾1ï¼šé›†å‡ä»·
#     plt.figure(figsize=(12, 8))
#     bars = plt.barh(top_avg["å‰§å"], top_avg["é›†å‡ä»·ï¼ˆå…ƒï¼‰"], color="skyblue")
#     plt.xlabel("é›†å‡ä»·ï¼ˆå…ƒï¼‰")
#     # plt.title(f"é›†å‡ä»· Top {top_n}")
#     plt.title(f"é›†å‡ä»· Top")
#     plt.gca().invert_yaxis()
#     plt.yticks(fontsize=10)
#     plt.tight_layout()
#     plt.savefig("é›†å‡ä»·_TOP.png", dpi=200)
#     plt.close()

#     # âœ… å›¾2ï¼šä»˜è´¹é›†å•ä»·
#     plt.figure(figsize=(12, 8))
#     bars = plt.barh(top_unit["å‰§å"], top_unit["ä»˜è´¹é›†å•ä»·ï¼ˆå…ƒï¼‰"], color="salmon")
#     plt.xlabel("ä»˜è´¹é›†å•ä»·ï¼ˆå…ƒï¼‰")
#     plt.title(f"ä»˜è´¹é›†å•ä»· Top")
#     # plt.title(f"ä»˜è´¹é›†å•ä»· Top {top_n}")
#     plt.gca().invert_yaxis()
#     plt.yticks(fontsize=10)
#     plt.tight_layout()
#     plt.savefig("ä»˜è´¹é›†å•ä»·_TOP.png", dpi=200)
#     plt.close()

#     print("ğŸ“Š å›¾è¡¨å·²ç”Ÿæˆï¼šé›†å‡ä»·_TOP.pngï¼Œä»˜è´¹é›†å•ä»·_TOP.png")

# def draw_price_charts(csv_file="å…¨éƒ¨ä»˜è´¹å¹¿æ’­å‰§_é›†å‡ä»·ç»Ÿè®¡.csv"):
#     df = pd.read_csv(csv_file)

#     # æ¸…æ´—æ•°æ®
#     df = df[pd.to_numeric(df["é›†å‡ä»·ï¼ˆå…ƒï¼‰"], errors='coerce').notna()]
#     df = df[pd.to_numeric(df["ä»˜è´¹é›†å•ä»·ï¼ˆå…ƒï¼‰"], errors='coerce').notna()]
#     df["é›†å‡ä»·ï¼ˆå…ƒï¼‰"] = df["é›†å‡ä»·ï¼ˆå…ƒï¼‰"].astype(float)
#     df["ä»˜è´¹é›†å•ä»·ï¼ˆå…ƒï¼‰"] = df["ä»˜è´¹é›†å•ä»·ï¼ˆå…ƒï¼‰"].astype(float)

#     # ä¸æˆªå–å‰ 30ï¼Œç›´æ¥å…¨é‡ç»˜å›¾
#     sorted_avg = df.sort_values("é›†å‡ä»·ï¼ˆå…ƒï¼‰", ascending=False)
#     sorted_unit = df.sort_values("ä»˜è´¹é›†å•ä»·ï¼ˆå…ƒï¼‰", ascending=False)

#     # å›¾1ï¼šå…¨éƒ¨é›†å‡ä»·
#     plt.figure(figsize=(12, max(6, 0.3 * len(sorted_avg))))
#     plt.barh(sorted_avg["å‰§å"], sorted_avg["é›†å‡ä»·ï¼ˆå…ƒï¼‰"], color="skyblue")
#     plt.xlabel("é›†å‡ä»·ï¼ˆå…ƒï¼‰")
#     plt.title("å…¨éƒ¨å¹¿æ’­å‰§ - é›†å‡ä»·")
#     plt.gca().invert_yaxis()
#     plt.yticks(fontsize=9)
#     plt.tight_layout()
#     plt.savefig("é›†å‡ä»·_ALL.png", dpi=200)
#     plt.close()

#     # å›¾2ï¼šå…¨éƒ¨ä»˜è´¹é›†å•ä»·
#     plt.figure(figsize=(12, max(6, 0.3 * len(sorted_unit))))
#     plt.barh(sorted_unit["å‰§å"], sorted_unit["ä»˜è´¹é›†å•ä»·ï¼ˆå…ƒï¼‰"], color="salmon")
#     plt.xlabel("ä»˜è´¹é›†å•ä»·ï¼ˆå…ƒï¼‰")
#     plt.title("å…¨éƒ¨å¹¿æ’­å‰§ - ä»˜è´¹é›†å•ä»·")
#     plt.gca().invert_yaxis()
#     plt.yticks(fontsize=9)
#     plt.tight_layout()
#     plt.savefig("ä»˜è´¹é›†å•ä»·_ALL.png", dpi=200)
#     plt.close()

#     print("ğŸ“Š å›¾è¡¨å·²ç”Ÿæˆï¼šé›†å‡ä»·_ALL.pngï¼Œä»˜è´¹é›†å•ä»·_ALL.png")

def draw_price_charts_paged(csv_file="å…¨éƒ¨ä»˜è´¹å¹¿æ’­å‰§_é›†å‡ä»·ç»Ÿè®¡.csv", page_size=50):
    df = pd.read_csv(csv_file)

    # æ¸…æ´—æ•°æ®
    df = df[pd.to_numeric(df["é›†å‡ä»·ï¼ˆå…ƒï¼‰"], errors='coerce').notna()]
    df = df[pd.to_numeric(df["ä»˜è´¹é›†å•ä»·ï¼ˆå…ƒï¼‰"], errors='coerce').notna()]
    df["é›†å‡ä»·ï¼ˆå…ƒï¼‰"] = df["é›†å‡ä»·ï¼ˆå…ƒï¼‰"].astype(float)
    df["ä»˜è´¹é›†å•ä»·ï¼ˆå…ƒï¼‰"] = df["ä»˜è´¹é›†å•ä»·ï¼ˆå…ƒï¼‰"].astype(float)

    # æŒ‰é›†å‡ä»·æ’åºå’Œåˆ†ç»„åˆ†é¡µ
    sorted_avg = df.sort_values("é›†å‡ä»·ï¼ˆå…ƒï¼‰", ascending=False).reset_index(drop=True)
    sorted_unit = df.sort_values("ä»˜è´¹é›†å•ä»·ï¼ˆå…ƒï¼‰", ascending=False).reset_index(drop=True)

    def plot_paged(data, value_col, title_prefix, color, filename_prefix):
        total = len(data)
        pages = (total + page_size - 1) // page_size

        for i in range(pages):
            start = i * page_size
            end = min((i + 1) * page_size, total)
            chunk = data.iloc[start:end]

            plt.figure(figsize=(12, 0.4 * len(chunk) + 2))
            plt.barh(chunk["å‰§å"], chunk[value_col], color=color)
            plt.xlabel(value_col)
            plt.title(f"{title_prefix}ï¼ˆç¬¬ {i+1} é¡µï¼Œå…± {pages} é¡µï¼‰")
            plt.gca().invert_yaxis()
            plt.yticks(fontsize=9)
            plt.tight_layout()
            plt.savefig(f"{filename_prefix}_Page_{i+1}.png", dpi=200)
            plt.close()

    # åˆ†é¡µç»˜åˆ¶ä¸¤ç±»å›¾
    plot_paged(sorted_avg, "é›†å‡ä»·ï¼ˆå…ƒï¼‰", "å¹¿æ’­å‰§é›†å‡ä»· Top", "skyblue", "é›†å‡ä»·")
    plot_paged(sorted_unit, "ä»˜è´¹é›†å•ä»·ï¼ˆå…ƒï¼‰", "å¹¿æ’­å‰§ä»˜è´¹é›†å•ä»· Top", "salmon", "ä»˜è´¹é›†å•ä»·")

    print("ğŸ“Š åˆ†é¡µå›¾è¡¨å·²ç”Ÿæˆï¼ˆå¦‚ï¼šé›†å‡ä»·_Page_1.pngï¼Œä»˜è´¹é›†å•ä»·_Page_1.png ç­‰ï¼‰")




if __name__ == "__main__":
    RUN_MAIN = True     # æ”¹ä¸º True å°±ä¼šè¿è¡Œ main()
    RUN_DRAW = False      # æ”¹ä¸º False å°±ä¸ç»˜å›¾

    if RUN_MAIN:
        main()

    if RUN_DRAW:
        # draw_price_charts()
        draw_price_charts_paged()
